# Lecture 10 (Thu 11/02/2023)

Owner: Philip Manfred Pincencia

### Revisit Gaussian

CDF is $F_X(u)=P(X\leq u)=\int_{-\infty}^u f_X(z)~dz$. 

Consider special case $X\sim N(0,1)$. pdf: $f_X(z)=\frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}}$. Since this integral is hard to calculate, we just name $\Phi(u)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^u e^{-\frac{z^2}{2}}$

How can we get the CDF of $X\sim N(m,\sigma^2)$ in terms of $\Phi(u)$?

$$
F_X(u)=\int_{-\infty}^u \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{z-\mu}{\sigma}\right)^2}dz
$$

Substitution with $w=\frac{z-\mu}{\sigma}, dw=\frac{dz}{\sigma}$. 

$$
=\int_{-\infty}^\frac{u-\mu}{\sigma}\frac{1}{\sqrt{2\pi}}e^{-\frac{w^2}{2}}dw=\Phi(\frac{u-\mu}{\sigma}) \\ \therefore F_X(u)=\Phi(\frac{u-\mu}{\sigma})
$$

*Note: Most computer languages have built-in function to help calculate $\Phi(u)$.* 

*Also, there is $erf(u)=\frac{2}{\pi}\int_0^u e^{-t^2}dt$ and $erfc(u)=1-erf(u)$.* 

*How to convert: **$\Phi(u)=\frac{1}{2}+\frac{1}{2}erf(\frac{u}{\sqrt{2}})$**.*

## Expected values of functions of random variables

Suppose $Y=g(X)$ where $X$ is a random variable and $g$ is a deterministic function. 

Want to find $E[Y]=E[g(X)]$. 

If $g(X)$ is continuous, $E[g(x)]=\int_{-\infty}^\infty g(u)f_X(u)~du$. 

If itâ€™s discrete, $E[g(X)]=\sum_u g(u)f_X(u)$. 

Example: Suppose $X$ is discrete with pmf below: 

Suppose $g(u)=u^2$. What is $E[g(u)]=E[X^2]$

We know $E[X]=-2a+b+2c$, hence $E[X^2]=(-2)^2\cdot a+1^2 \cdot b+2^2\cdot c=4(a+c)+b$

Example: Suppose $g(u)=au+b$, where $a,b\in\mathbb{R}$. Want $E[g(X)]=E[aX+b]$

$$
E[aX+b]=\int_{-\infty}^\infty (au+b)f_X(u)~du\\=a\int_{-\infty}^\infty  uf_X(u)du+b\int_{-\infty}^\infty f_X(u)du=aE[X]+b
$$

Same kind of proof for discrete case. 

> $E[a\cdot g_1(X)+b\cdot g_2(X)]=aE[g_1(X)]+bE[g_2(X)]$
> 

**Definition:** 

The **nth moment** of a random variable $X$ is defined to be $E[X^n]$. 

The **nth central moment** of $X$ is $E[(X-E[X])^n]$,          for $n\in\mathbb{Z^+}$

Facts: 1st moment is $E[X]=\mu$

- 1st central moment is $E[X-E[X]]=E[X]-E[E[X]]=E[X]-E[X]=0$
Note: $E[E[X]]=E[X]$
- 2nd central moment is called variance $\sigma_X^2\equiv$  $E[(X-E[X])^2]$ and also standard deviation is $\sigma_X$.

Intuitively, small variance: tall, thin pdfs. Large variance: short, fat pdfs. 

How can we compute the variance?

$\sigma_X^2=E[(X-\mu)^2]=\int_{-\infty}^\infty (u-\mu)^2f_X(u)du$. If $X$ is discrete, $\sigma_X^2=\sum_u (u-\mu)^2f_X(u)$. 

*Another (easier) approach:* 

$\sigma_X^2=E[(X-\mu)^2]=E[X^2-2\mu X+\mu^2]=E[X^2]+E[-2\mu X]+E[\mu^2]=E[X^2]-2\mu E[X]+\mu^2=E[X^2]-2\mu^2+\mu^2=E[X^2]-\mu^2\\ \therefore \sigma_X^2=E[X^2]-(E[X])^2$

Example: Compute variance of a random variable that is uniform on $[a,b]$. 

So, we know $f_X(u)=\frac{1}{b-a}$ and $E[X]=\frac{a+b}{2}$. 

Hence, $E[X^2]=\int_a^b u^2\frac{1}{b-a}~du=\frac{b^3-a^3}{3(b-a)}=\frac{(b-a)(b^2+ab+a^2)}{3(b-a)}=\frac{b^2+ab+a^2}{3}$

So, $\sigma_X^2=E[X^2]-\mu^2=\frac{1}{3}(b^2+ab+a^2)-\frac{1}{4}(a^2+ab+b^2)=\frac{(a-b)^2}{12}=\frac{\delta^2}{12}$, where $\delta$ is the width of the interval. 

Example: Let $X$ be a binary random variable with $p_X(1)=q=1-p_X(0)$. Find variance:

So, $\sigma_X^2=E[X^2]-(E[X])^2=\sum_u u^2\cdot p_X(u) - \sum_u u\cdot p_X(u)=q-q^2=q(1-q)$

In extreme case where $q\approx 0$, we have a random variable that is nearly deterministic. i.e. it always equal to $0$.