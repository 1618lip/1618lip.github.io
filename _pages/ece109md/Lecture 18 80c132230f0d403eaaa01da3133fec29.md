# Lecture 18

Owner: Philip Manfred Pincencia

# Limit Theorems

Law of Large Numbers

Suppose we observe $n$ independent values of a random values of $X$. Call those $x_1, x_2,…,x_n$

We wanna know the empirical mean (observed/sample mean)

$\frac{x_1+ x_2+…+x_n}{n}$. 

We might use this as an estimate of the true mean (which you don’t know), $E[X]$. 

Want to prove formally that 

 

$$
\lim_{n\to \infty} \frac{x_1+ x_2+…+x_n}{n} = E[X]
$$

Pretend $x_1, x_2,…,x_n$ are themselves random variables. Then, we can let $Z=\frac{x_1+ x_2+…+x_n}{n}$ be a random variable. 

We will model the observations $x_1, x_2,…,x_n$ of random variables $X$ as iid random variables with same pdf/pmf as $X$. What does it mean for a random variable to approach a number (a.k.a become deterministic or variance is 0). 

Specifically, we will show that for any $\epsilon > 0$, 

$$
\lim_{n\to \infty} P(|\frac{x_1+ x_2+…+x_n}{n}- E[X]| >\epsilon) = 0
$$

To prove this, we need two inequalities: 

### Markov Inequality

Let $X$ be a nonnegative random variable with $E[X] < \infty$. Then for all $\alpha > 0$, $P(X\geq a)\leq \frac{E[X]}{\alpha}$. 
Proof: 
$P(X\geq a=\int_\alpha^\infty f_X(u)~du$
Note: In this integral, $\frac{u}{\alpha}\geq 1$, since $u\geq a$ by the integral bounds. 
So, 

$$
P(X\geq a)=\int_\alpha^\infty f_X(u)~du \leq \int_\alpha^\infty \frac{u}{\alpha}f_X(u)~du=\frac{1}{\alpha}\int_\alpha^\infty uf_X(u)~du\leq \frac{1}{\alpha}\int_0^\infty uf_X(u)~du=\frac{E[X]}{\alpha}
$$

### Chebychev Inequality

Let $X$ be a random variable with $E[X] <\infty$ and $\sigma_X^2 < \infty$. Then, for all $t> 0$, $P(|X-E[X]|\geq t)\leq \frac{\sigma_X^2}{t^2}$ (takeaway: the probability $\propto \frac{1}{t^2})$

Proof: 
Apply Markov inequality to the nonnegative random variable $(X-E[X])^2$

$$
P(|X-E[X]|\geq t)=P((X-E[X])^2\geq t^2)\leq \frac{E[(X-E[X])^2]}{t^2}=\frac{\sigma_X^2}{t^2}
$$

Now, let’s prove Law Large Numbers: 

Let $Y_n=\frac{X_1+…+X_n}{n}=$ empirical mean. Want to show $Y_n\to \mu$ as $n\to \infty$. 

1. Use Chebychev inequality: 

$$
E[Y_n]=E[\frac{X_1+…+X_n}{n}]=\frac{1}{n}(E[X_1]+...+E[X_n])=\frac{1}{n}\cdot n\mu=\mu,~\text{for all}~n
$$

Variance of $Y_n$: 

$$
Var(Y_n)=Var(\frac{X_1+…+X_n}{n})=\frac{1}{n^2}Var(X_1+...+X_n)=\frac{1}{n^2}[Var(X_1)+...+Var(X_n)]=\frac{1}{n^2}\cdot n\sigma_X^2=\frac{\sigma_X^2}{n}
$$

And as $\lim_{n\to\infty} \frac{\sigma_X^2}{n}=0$, which is what we want. 

*Apply Chebychev:* 

For any $t>0$, 

$$
P(|Y_n-E[Y_n]\geq t)\leq \frac{Var(Y_n)}{t^2}=\frac{\sigma_X^2}{nt^2}
$$

For any $t>0$, RHS goes to zero as $n\to\infty$.

# Central Limit Theorem

We know from Law of Large Numbers that

$Y_n=\frac{X_1+…+X_n}{n}$ looks like the mean $E[X]$ as $n\to \infty$. 

This theorem tells us as remarkable fact. Namely, the shape of the CDF of $Y_n$ starts to look Gaussian as $n\to\infty$. 

Let $X_1,…,X_n$ be iid random variables with mean $\mu$ and variance $\sigma^2$. Let $S_n=X_1+…+X_n$. Form a new random variable, $Y_n=\frac{S_n-n\mu}{\sigma\sqrt{n}}$ (zero mean random variable with variance = 1). $Y_n=\frac{S_n-n\mu}{\sigma\sqrt{n}}=\frac{S_n-E[S_n]}{\sigma_{S_n}}$. 

CLT says CDF of $Y_n$ converges to the CDF of $N(0,1)$ Gaussian. Recall notation $\Phi(u)=$ CDF of $N(0,1)$. 

i.e. $\lim_{n\to\infty} F_{Y_n}(u)=\Phi(u)$ “converges in distribution”