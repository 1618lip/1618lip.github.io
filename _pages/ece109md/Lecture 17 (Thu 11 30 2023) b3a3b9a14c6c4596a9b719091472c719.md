# Lecture 17 (Thu 11/30/2023)

Owner: Philip Manfred Pincencia

## Covariance

Fact: If $X,Y$ are independent random variables, then $X,Y$ are uncorrelated. 

*Proof:* 

Compute $E[XY]=\int_{-\infty}^\infty \int_{-\infty}^\infty uv\cdot f_{X,Y}(u,v)~dudv$. Assume they’re independent, then 

$$
E[XY]=\int_{-\infty}^\infty uf_X(u)~du\cdot \int_{-\infty}^\infty vf_{Y}(v)~dv=E[X]E[Y]
$$

Since $Cov(X,Y)=E[XY]-E[X]E[Y]=0~\square$

But, the *converse is generally false*. It’s possible they are uncorrelated, but not independent. 

Uncorrelated means $Cov(X,Y)=0$

### Variances of sums

$Var(X+Y)=E[(X+Y)^2]-E[X+Y]^2=E[X^2]+2E[X+Y]+E[Y^2]$

$$
Var(X+Y)=E[(X+Y)^2]-E[X+Y]^2\\=E[X^2]+2E[X+Y]+E[Y^2]-(E[X]+E[Y])^2\\=E[X^2]-E[X]^2+E[Y^2]-E[Y]^2+2(E[XY]-E[X]E[Y])\\=Var(X)+Var(Y)+2Cov(X,Y)
$$

> $Var(X\pm Y)=Var(X)+Var(Y)\pm 2Cov(X,Y)$
> 

Special case: If $X,Y$ are uncorrelated, then $Var(X\pm Y)=Var(X)\pm Var(Y)$. In particular, true if $X,Y$ are independent. 

*Fact:* $|\rho_{X,Y}|\leq 1$

Proof: 

We know $0\leq Var(\frac{X}{\sigma_X}\pm \frac{Y}{\sigma_Y})=Var(\frac{X}{\sigma_X})+Var(\frac{Y}{\sigma_Y})\pm 2Cov(\frac{X}{\sigma_X},\frac{Y}{\sigma_Y})\\=\frac{1}{\sigma_X^2}Var(X)+\frac{1}{\sigma_Y^2}Var(Y)\pm \frac{2}{\sigma_X\sigma_Y}Cov(X,Y)=1+1 \pm2\rho_{X,Y}=2(1\pm \rho_{X,Y})$ . 

Hence, $1\pm \rho_{X,Y}\geq 0~\to~-1\leq \rho_{X,Y}\leq 1$

---

**Importance of** $\rho_{X,Y}$: *For prediction*

Suppose $X,Y$ have a joint pdf or a joint pmf. We can only observe $Y$, but we want to know or estimate $X$. 

Form an estimate of $X$ as a deterministic function of $Y$: 

$$
\hat X=g(Y)
$$

One way is a ‘linear’ estimate: $\hat X=aY$, where $a$ is a constant. What’s the best choice of $a$? 

Let’s define one type of ‘best’? → Want $\hat X\approx X$ → (popular) Minimum Mean Squared Error

$e=X-\hat X$. Make $e$ small by making $e^2$ small on average (we square just for convenience sake). 

$e$ is a random variable. Estimate 

$$
E[e^2]=E[(X-\hat X)^2]=E[(X-aY)^2]=E[X^2]-2aE[XY]+a^2E[Y^2]
$$

Minimize over $a$. 

$$
\frac{d}{da}E[e^2]=-2E[XY]+2aE[Y^2]=0\to \mathbf{a=\frac{E[XY]}{E[Y^2]}}=\text{best choice of }a
$$

Special case: 

If $\sigma_X=\sigma_Y=1$ and $E[X]=E[Y]=0$, $E[Y^2]=1$ since $\sigma_Y^2=E[Y^2]-E[Y]^2$. 

Then $a=E[XY]=\rho_{X,Y}$ in this case. 

> Best linear estimate: $\hat X=\rho_{X,Y}\cdot Y$.
> 

## Recall: Binomial pmf

$$
p_X(k)=\binom{n}{k} p^k (1-p)^{n-k}
$$

We got $E[X]=np$ using summation. 

Alternative derivation: 

Let $Y_i=\begin{cases} 1 & \text{if get heads at ith flip} \\ 0 & \text{tails} \end{cases}$.  The random variables $Y_1, …, Y_n$ are iid. 

Can write: $X=Y_1+Y_2+…+Y_n$

So, $E[X]=E[\sum_{k=1}^n Y_i]=\sum_{k=1}^nE[Y_i]=nE[Y_1]=n(1\cdot p(H)+0\cdot p(T))=np$

Recall Gaussian random variables

What if $X,Y$ are both Gaussian, but not independent. 

*Def:* Random Variables $X,Y$ are called jointly Gaussian if their joint pdf is of the following form 

$$
f_{X,Y}(u,v)=\frac{1}{2\pi \sigma_X\sigma_Y\sqrt{1-\rho^2}}\cdot e^{-\frac{1}{2(1-\rho^2)}[(\frac{u-\mu_X}{\sigma_X})^2+(\frac{v-\mu_Y}{\sigma_Y})^2-2\rho (\frac{u-\mu_X}{\sigma_X})(\frac{v-\mu_Y}{\sigma_Y})]}
$$

, where $\rho$ turns out to be the $\rho_{X,Y}$. 

*Special case:* $\mu_X=\mu_Y=0$ and $\sigma_X=\sigma_Y=1$. 

Then, 

$$
f_{X,Y}(u,v)=\frac{1}{2\pi \sqrt{1-\rho^2}}\cdot e^{-\frac{1}{2(1-\rho^2)}[u^2+v^2-2\rho uv]}
$$

If $\rho=0$, then 

$$
f_{X,Y}(u,v)=\frac{1}{2\pi}\cdot e^{-\frac{1}{2}(u^2+v^2)}=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}u^2}\cdot \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}v^2}
$$

You can generalize to $n$ jointly Gaussian random variables. 

Important fact: If $X,Y$ are jointly Gaussian and $Z=aX+bY$ and $W=cX+dY$, then $Z,W$ are also jointly Gaussian. 

Find $\mu_Z,\mu_W,\sigma_Z^2,\sigma_W^2, \rho_{Z,W}$ in terms of $\mu_X, \mu_Y, \sigma_X^2, \sigma_Y^2, \rho_{X,Y}$. 

$$Z